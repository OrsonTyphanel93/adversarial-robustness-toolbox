# MIT License
#
# Copyright (C) The Adversarial Robustness Toolbox (ART) Authors 2022
#
# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
# documentation files (the "Software"), to deal in the Software without restriction, including without limitation the
# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit
# persons to whom the Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the
# Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE
# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""
This module implements Clean Label Backdoor Attacks to poison data used in ML models ( Audios )
"""

import logging
from typing import Callable, Optional, Tuple, Union


import numpy as np


from art.attacks.attack import PoisoningAttackBlackBox


logger = logging.getLogger(__name__)


class PoisoningAttackDynamicCleanLabelBackdoor(PoisoningAttackBlackBox):
   """
   A clean-label dynamic backdoor attack that injects a trigger into clean data samples of a specific target class.


   This attack is useful for poisoning machine learning models with a backdoor trigger while maintaining the original labels.


   :param trigger_func: A callable function that applies the backdoor trigger to input data samples dynamically.
   :param target_label: The target class label to inject the backdoor trigger into.
   """


   attack_params = PoisoningAttackBlackBox.attack_params + ["trigger_func", "target_label"]
   _estimator_requirements = ()


   def __init__(self, trigger_func: Callable, target_label: Union[int, str]) -> None:
       """
       Initialize a CleanLabelDynamicBackdoorAttack instance.


       :param trigger_func: A callable function that applies the backdoor trigger to input data samples dynamically.
       :param target_label: The target class label to inject the backdoor trigger into.
       """
       super().__init__()
       self.trigger_func = trigger_func
       self.target_label = target_label
       self._check_params()


   def poison(
       self, x: np.ndarray, y: np.ndarray, broadcast: bool = False, **kwargs
   ) -> Tuple[np.ndarray, np.ndarray]:
       """
       Inject a dynamic backdoor trigger into clean data samples of the target class.


       :param x: Clean data samples.
       :param y: True labels for the clean data samples.
       :param broadcast: If True, broadcast target labels to match the number of samples in x.
       :param kwargs: Additional keyword arguments (not used).


       :return: Poisoned data samples with clean labels.
       """
       if self.target_label not in y:
           # If the target label is not present in y, return the original data and labels
           return x, y


       num_poison = len(x)
       if num_poison == 0:
           raise ValueError("Must input at least one poison point.")
       poisoned = np.copy(x)


       if callable(self.trigger_func):
           # Apply the dynamic trigger function to the input data
           trigger_samples = x[y == self.target_label]
           poisoned[y == self.target_label] = self.trigger_func(trigger_samples)


       return poisoned, y


   def _check_params(self) -> None:
       if not callable(self.trigger_func):
           raise ValueError("Trigger function must be callable.")
       if not isinstance(self.target_label, (int, str)):
           raise ValueError("Target label must be an integer or string.")

