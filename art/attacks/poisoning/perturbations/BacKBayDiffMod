# MIT License
#
# Copyright (C) The Adversarial Robustness Toolbox (ART) Authors 2020
#
# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
# documentation files (the "Software"), to deal in the Software without restriction, including without limitation the
# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit
# persons to whom the Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the
# Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE
# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
"""
This module implements an audio backdoor attack using a diffusion model and Bayesian techniques. This attack generates a contradictory sound that can be printed in the physical world using HugginFace's pre-trained ASR audio model. 

| 
"""
from art.estimators.classification import TensorFlowV2Classifier

import matplotlib.pyplot as plt
import torch
import torchvision
import transformers

import warnings
warnings.filterwarnings("ignore")

tqdm.pandas()
from art.estimators.classification.hugging_face import HuggingFaceClassifierPyTorch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
from typing import Any, Callable, Optional, Union, Tuple
import numpy as np
import scipy.stats as stats
import pymc as pm
import matplotlib.pyplot as plt

# Constants for magic numbers
DEFAULT_FLIP_PROB = 0.5
DEFAULT_TRIGGER_ALPHA = 0.1
DEFAULT_POISON_RATE = 0.01

class PoisoningAttackCleanLabelBackdoor:
    """
    This class implements a poisoning attack with a clean label backdoor.
    """

    def __init__(
        self,
        trigger_func: Callable,
        target_label: Union[int, str, np.ndarray],
        dirty_label: Union[int, str, np.ndarray],
        flip_prob: float = DEFAULT_FLIP_PROB,
        trigger_alpha: float = DEFAULT_TRIGGER_ALPHA,
        poison_rate: float = DEFAULT_POISON_RATE,
        backdoor_trigger: Optional[Union[int, str, np.ndarray]] = None,
        backdoor_target: Optional[Union[int, str, np.ndarray]] = None,
        training_dataset: Optional[np.ndarray] = None,
        training_params: Optional[dict] = None,
        prior_mean: float = 0,
        prior_std: float = 1
    ) -> None:
        """
        Initialize the PoisoningAttackCleanLabelBackdoor instance.
        """
        self.trigger_func = trigger_func
        self.target_label = target_label
        self.dirty_label = dirty_label
        self.flip_prob = flip_prob
        self.trigger_alpha = trigger_alpha
        self.poison_rate = poison_rate
        self.backdoor_trigger = backdoor_trigger if backdoor_trigger is not None else 0  # Initialize backdoor_trigger
        self.backdoor_target = backdoor_target
        self.training_dataset = training_dataset
        self.training_params = training_params
        self.prior_mean = prior_mean
        self.prior_std = prior_std
        self.prior = stats.norm(loc=prior_mean, scale=prior_std)

    def poison(
        self,
        x_audio: np.ndarray,
        y: Optional[np.ndarray] = None,
        broadcast: bool = False,
        random_seed: Optional[int] = None
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Apply the poisoning attack to the given audio data.
        """
        if y is None or not np.any(np.isin(self.target_label, y)):
            return x_audio, y

        num_poison = int(len(x_audio) * self.poison_rate)

        poisoned_labels = np.full((num_poison,), self.dirty_label)

        if broadcast:
            y_attack = np.broadcast_to(y, (x_audio.shape[0], y.shape[0]))
        else:
            y_attack = np.copy(y)

        np.random.seed(random_seed)

        for i in range(num_poison):
            trigger_pattern = self.trigger_func(x_audio[i])

            if np.random.rand() < self.flip_prob:
                poisoned_labels[i] = self.target_label[0]

            x_audio[i] = (1 - self.trigger_alpha) * x_audio[i] + self.trigger_alpha * trigger_pattern

        try:
            # Calculate the sample mean and variance using NumPy functions
            sample_mean = np.mean(x_audio)
            sample_variance = np.var(x_audio)

            # Update the prior with the sample statistics
            self.prior = stats.norm(loc=sample_mean, scale=np.sqrt(sample_variance))

            # Perform Bayesian sampling
            _ = self._bayesian_sampling_diffusion_model(T=10, alpha=np.linspace(0.1, 1.0, 10), beta=np.linspace(0.2, 2.0, 10), sigma=np.linspace(0.3, 3.0, 10), noise_dist=np.random.normal)

        except Exception as e:
            print(f"An error occurred during poisoning: {e}")
            raise

        return x_audio, poisoned_labels

    def _fokker_planck(
        self,
        x: float,
        t: int,
        alpha: np.ndarray,
        beta: np.ndarray,
        sigma: np.ndarray,
        noise_dist: Callable[[Any], np.ndarray]
    ) -> float:
        """
        Calculate the Fokker-Planck equation for a given x, t, alpha, beta, sigma, and noise distribution.
        """
        assert isinstance(x, float), "Expected x to be a float"
        assert isinstance(t, int), "Expected t to be an integer"
        assert isinstance(alpha, np.ndarray) and alpha.ndim == 1, "Expected alpha to be a 1D numpy array"
        assert isinstance(beta, np.ndarray) and beta.ndim == 1, "Expected beta to be a 1D numpy array"
        assert isinstance(sigma, np.ndarray) and sigma.ndim == 1, "Expected sigma to be a 1D numpy array"
        # Ensure that the index is within bounds
        t = min(t, len(alpha) - 1)
        #return np.sqrt(alpha[t]) * (x - np.sqrt(1 - alpha[t]) * noise_dist(beta[t])) + sigma[t] * noise_dist(0)
        return np.exp(np.sqrt(alpha[t])) * (x - np.exp(np.sqrt(1 - alpha[t])) * noise_dist(beta[t])) + sigma[t] * noise_dist(0)

    def _bayesian_sampling_diffusion_model(
        self,
        T: int,
        alpha: np.ndarray,
        beta: np.ndarray,
        sigma: np.ndarray,
        noise_dist: Callable[[Any], np.ndarray]
    ) -> pm.backends.base.MultiTrace:
        """
        Perform bad diffusion sampling for a given time period, alpha, beta, sigma, and noise distribution.

        Parameters:
        - T (int): Time period for sampling.
        - alpha (np.ndarray): Array of alpha values.
        - beta (np.ndarray): Array of beta values.
        - sigma (np.ndarray): Array of sigma values.
        - noise_dist (Callable[[Any], np.ndarray]): Noise distribution function.

        Returns:
        pm.backends.base.MultiTrace: Trace of the bad diffusion sampling.
        """
        assert isinstance(T, int), "Expected T to be an integer"
        assert isinstance(alpha, np.ndarray) and alpha.ndim == 1, "Expected alpha to be a 1D numpy array"
        assert isinstance(beta, np.ndarray) and beta.ndim == 1, "Expected beta to be a 1D numpy array"
        assert isinstance(sigma, np.ndarray) and sigma.ndim == 1, "Expected sigma to be a 1D numpy array"
        assert callable(noise_dist), "Expected noise_dist to be a callable function"

        try:

           with pm.Model() as model:
            # Initialize the state variable x_T
            x_T = pm.Normal('x_T', mu=self.prior.rvs() if np.random.rand() < self.poison_rate else noise_dist(self.backdoor_trigger), sigma=1)

            # Iterate backward in time
            for t in range(T - 1, -1, -1):
                z = noise_dist(0) if t > 1 else 0
                x_t_minus_1 = pm.Normal(f'x_{t}', mu=np.sqrt(alpha[t]) * (x_T - np.sqrt(1 - alpha[t]) * noise_dist(beta[t])) + sigma[t] * z, sigma=1)
                x_T = x_t_minus_1

            # Use PyMC for Bayesian sampling
                # No need to define likelihood for deterministic variables

                # Sample using NUTS
                trace = pm.sample(2000, tune=1000, cores=2, chains=2, step=pm.NUTS()) # pm.Metropolis
                trace.sample_stats

            return trace

        except Exception as e:
            print(f"An error occurred: {e}")
            raise

    """
